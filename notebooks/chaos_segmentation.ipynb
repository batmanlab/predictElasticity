{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#DICOM-Images\" data-toc-modified-id=\"DICOM-Images-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>DICOM Images</a></span></li><li><span><a href=\"#Nifti-Maker\" data-toc-modified-id=\"Nifti-Maker-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Nifti Maker</a></span></li><li><span><a href=\"#xarray-generation\" data-toc-modified-id=\"xarray-generation-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>xarray generation</a></span></li><li><span><a href=\"#xarray-viewer\" data-toc-modified-id=\"xarray-viewer-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>xarray viewer</a></span></li><li><span><a href=\"#Model-Training\" data-toc-modified-id=\"Model-Training-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Model Training</a></span></li><li><span><a href=\"#Slurm-Analysis\" data-toc-modified-id=\"Slurm-Analysis-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Slurm Analysis</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DICOM Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import SimpleITK as sitk\n",
    "\n",
    "import holoviews as hv\n",
    "from holoviews import opts\n",
    "import panel as pn\n",
    "import hvplot.pandas\n",
    "hv.extension('bokeh')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (10,8)\n",
    "\n",
    "from mre.plotting import patient_series_viewer, chaos_viewer, xr_viewer, hv_dl_vis_chaos\n",
    "from mre.preprocessing import make_nifti_atlas_v2, make_xr_dataset_for_chaos\n",
    "from mre.segmentation import ChaosDataset\n",
    "from mre.train_seg_model import train_seg_model \n",
    "from mre import pytorch_arch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.utils\n",
    "from torchsummary import summary\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import models\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/pghbio/dbmi/batmanlab/bpollack/predictElasticity/data/CHAOS/Train_Sets/MR/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# patient_series_viewer(data_dir, 'DICOMA/PA1/ST0')\n",
    "# patient_series_viewer(data_dir, '1', img_type='DICOM_CHAOS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nifti Maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make_nifti_atlas_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#patient_series_viewer(data_dir, 'NIFTI/01', img_type='NIFTI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chaos_viewer(data_dir, 'NIFTI/03')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xarray generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls ../data/CHAOS/Train_Sets/MR/NIFTI/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patients = [\"01\",  \"03\",  \"08\",  \"13\",  \"19\",  \"21\",  \"31\",  \"33\",  \"36\",  \"38\",\n",
    "# \"02\",  \"05\",  \"10\",  \"15\",  \"20\",  \"22\",  \"32\",  \"34\",  \"37\",  \"39\"] \n",
    "# ds = make_xr_dataset_for_chaos(patients, 256, 256, 32, 'chaos')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xarray viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_path = Path(data_dir, 'xarray_chaos.nc')\n",
    "ds = xr.open_dataset(ds_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr_viewer(ds, overlay_data='mask')\n",
    "#ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths\n",
    "out_dir = '/pghbio/dbmi/batmanlab/bpollack/predictElasticity/data/CHAOS/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-15_12-08-14\n",
      "{'train_trans': True, 'train_clip': True, 'train_aug': False, 'train_sample': 'shuffle', 'val_trans': True, 'val_clip': True, 'val_aug': False, 'val_sample': 'shuffle', 'test_trans': True, 'test_clip': True, 'test_aug': False, 'train_seq_mode': None, 'val_seq_mode': None, 'test_seq_mode': 'all', 'def_seq_mode': 'random', 'seed': 100, 'subj': '01', 'batch_size': 100, 'model_cap': 16, 'lr': 0.05, 'step_size': 80, 'gamma': 0.1, 'num_epochs': 200, 'dry_run': False, 'coord_conv': False, 'loss': 'dice', 'model_arch': '3D', 'n_layers': 5, 'in_channels': 1, 'out_channels_final': 1, 'channel_growth': True, 'transfer_layer': False, 'bce_weight': 0.5, 'resize': False, 'transform': True, 'bc_weight': 0.2}\n",
      "<xarray.Dataset>\n",
      "Dimensions:   (sequence: 3, subject: 20, x: 256, y: 256, z: 32)\n",
      "Coordinates:\n",
      "  * subject   (subject) object '01' '03' '08' '13' '19' ... '32' '34' '37' '39'\n",
      "  * sequence  (sequence) object 't1_in' 't1_out' 't2'\n",
      "  * x         (x) int64 0 1 2 3 4 5 6 7 8 ... 248 249 250 251 252 253 254 255\n",
      "  * y         (y) int64 255 254 253 252 251 250 249 248 247 ... 7 6 5 4 3 2 1 0\n",
      "  * z         (z) int64 0 1 2 3 4 5 6 7 8 9 10 ... 22 23 24 25 26 27 28 29 30 31\n",
      "Data variables:\n",
      "    image     (subject, sequence, x, y, z) int16 ...\n",
      "    mask      (subject, sequence, x, y, z) int16 ...\n",
      "train:  16\n",
      "val:  3\n",
      "test:  1\n",
      "3d\n",
      "Let's use 4 GPUs!\n",
      "Epoch 0/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.786736, dice: 0.946531, loss: 0.866633\n",
      "val: bce: 2083031744512.000000, dice: 0.999986, loss: 1041515872256.000000\n",
      "saving best model\n",
      "0m 18s\n",
      "Epoch 1/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.624613, dice: 0.942063, loss: 0.783338\n",
      "val: bce: 454035689701376.000000, dice: 0.999986, loss: 227017844850688.000000\n",
      "0m 3s\n",
      "Epoch 2/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.446248, dice: 0.928019, loss: 0.687133\n",
      "val: bce: 17918995726336.000000, dice: 0.999986, loss: 8959497863168.000000\n",
      "0m 4s\n",
      "Epoch 3/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.308755, dice: 0.916853, loss: 0.612804\n",
      "val: bce: 123089739776.000000, dice: 0.999986, loss: 61544869888.000000\n",
      "saving best model\n",
      "0m 4s\n",
      "Epoch 4/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.231307, dice: 0.897521, loss: 0.564414\n",
      "val: bce: 4147225344.000000, dice: 0.999986, loss: 2073612672.000000\n",
      "saving best model\n",
      "0m 4s\n",
      "Epoch 5/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.175525, dice: 0.905254, loss: 0.540389\n",
      "val: bce: 6169244672.000000, dice: 0.999986, loss: 3084622336.000000\n",
      "0m 4s\n",
      "Epoch 6/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.148703, dice: 0.889067, loss: 0.518885\n",
      "val: bce: 15691574.000000, dice: 0.999986, loss: 7845787.500000\n",
      "saving best model\n",
      "0m 4s\n",
      "Epoch 7/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.127901, dice: 0.867813, loss: 0.497857\n",
      "val: bce: 5609149.500000, dice: 0.999986, loss: 2804575.250000\n",
      "saving best model\n",
      "0m 4s\n",
      "Epoch 8/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.113380, dice: 0.848904, loss: 0.481142\n",
      "val: bce: 6239199.500000, dice: 0.999986, loss: 3119600.250000\n",
      "0m 4s\n",
      "Epoch 9/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.107477, dice: 0.872796, loss: 0.490137\n",
      "val: bce: 562380.687500, dice: 0.999986, loss: 281190.843750\n",
      "saving best model\n",
      "0m 4s\n",
      "Epoch 10/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.103416, dice: 0.843305, loss: 0.473360\n",
      "val: bce: 55510.500000, dice: 0.999986, loss: 27755.750000\n",
      "saving best model\n",
      "0m 4s\n",
      "Epoch 11/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.101761, dice: 0.819408, loss: 0.460585\n",
      "val: bce: 5275.060059, dice: 0.999986, loss: 2638.030029\n",
      "saving best model\n",
      "0m 4s\n",
      "Epoch 12/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.121660, dice: 0.879029, loss: 0.500345\n",
      "val: bce: 19658.054688, dice: 0.999986, loss: 9829.527344\n",
      "0m 3s\n",
      "Epoch 13/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.097325, dice: 0.820700, loss: 0.459013\n",
      "val: bce: 36029.796875, dice: 0.999986, loss: 18015.398438\n",
      "0m 4s\n",
      "Epoch 14/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.092203, dice: 0.828699, loss: 0.460451\n",
      "val: bce: 45258.949219, dice: 0.999986, loss: 22629.974609\n",
      "0m 4s\n",
      "Epoch 15/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.094498, dice: 0.812461, loss: 0.453480\n",
      "val: bce: 25607.585938, dice: 0.999986, loss: 12804.292969\n",
      "0m 3s\n",
      "Epoch 16/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.093845, dice: 0.797667, loss: 0.445756\n",
      "val: bce: 6676.397461, dice: 0.999986, loss: 3338.698730\n",
      "0m 4s\n",
      "Epoch 17/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.089137, dice: 0.775922, loss: 0.432529\n",
      "val: bce: 2239.910156, dice: 0.999986, loss: 1120.455078\n",
      "saving best model\n",
      "0m 4s\n",
      "Epoch 18/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.088274, dice: 0.758550, loss: 0.423412\n",
      "val: bce: 906.436218, dice: 0.999986, loss: 453.718109\n",
      "saving best model\n",
      "0m 4s\n",
      "Epoch 19/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.082024, dice: 0.737506, loss: 0.409765\n",
      "val: bce: 377.397156, dice: 0.999986, loss: 189.198578\n",
      "saving best model\n",
      "0m 4s\n",
      "Epoch 20/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.076389, dice: 0.746184, loss: 0.411287\n",
      "val: bce: 229.612900, dice: 0.999986, loss: 115.306442\n",
      "saving best model\n",
      "0m 5s\n",
      "Epoch 21/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.084576, dice: 0.726890, loss: 0.405733\n",
      "val: bce: 103.998863, dice: 0.999986, loss: 52.499424\n",
      "saving best model\n",
      "0m 4s\n",
      "Epoch 22/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.083873, dice: 0.720978, loss: 0.402426\n",
      "val: bce: 109.717651, dice: 0.999986, loss: 55.358818\n",
      "0m 4s\n",
      "Epoch 23/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.098636, dice: 0.752907, loss: 0.425772\n",
      "val: bce: 108.513336, dice: 0.999986, loss: 54.756660\n",
      "0m 4s\n",
      "Epoch 24/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.074714, dice: 0.718399, loss: 0.396556\n",
      "val: bce: 95.889252, dice: 0.999986, loss: 48.444618\n",
      "saving best model\n",
      "0m 4s\n",
      "Epoch 25/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.073822, dice: 0.708270, loss: 0.391046\n",
      "val: bce: 67.355743, dice: 0.999986, loss: 34.177864\n",
      "saving best model\n",
      "0m 4s\n",
      "Epoch 26/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.075074, dice: 0.701409, loss: 0.388242\n",
      "val: bce: 51.940536, dice: 0.999986, loss: 26.470261\n",
      "saving best model\n",
      "0m 3s\n",
      "Epoch 27/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.074064, dice: 0.692075, loss: 0.383069\n",
      "val: bce: 35.357372, dice: 0.999986, loss: 18.178679\n",
      "saving best model\n",
      "0m 4s\n",
      "Epoch 28/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.069851, dice: 0.678198, loss: 0.374025\n",
      "val: bce: 16.851532, dice: 0.999986, loss: 8.925759\n",
      "saving best model\n",
      "0m 4s\n",
      "Epoch 29/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.065523, dice: 0.650629, loss: 0.358076\n",
      "val: bce: 7.246763, dice: 0.999986, loss: 4.123374\n",
      "saving best model\n",
      "0m 4s\n",
      "Epoch 30/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.064225, dice: 0.638111, loss: 0.351168\n",
      "val: bce: 3.751092, dice: 0.999986, loss: 2.375539\n",
      "saving best model\n",
      "0m 4s\n",
      "Epoch 31/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.061619, dice: 0.622925, loss: 0.342272\n",
      "val: bce: 1.933635, dice: 0.999984, loss: 1.466810\n",
      "saving best model\n",
      "0m 4s\n",
      "Epoch 32/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.062662, dice: 0.614099, loss: 0.338381\n",
      "val: bce: 1.075411, dice: 0.999004, loss: 1.037207\n",
      "saving best model\n",
      "0m 4s\n",
      "Epoch 33/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.059701, dice: 0.598948, loss: 0.329325\n",
      "val: bce: 0.657428, dice: 0.982966, loss: 0.820197\n",
      "saving best model\n",
      "0m 4s\n",
      "Epoch 34/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.058542, dice: 0.586408, loss: 0.322475\n",
      "val: bce: 0.384970, dice: 0.935212, loss: 0.660091\n",
      "saving best model\n",
      "0m 4s\n",
      "Epoch 35/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.060085, dice: 0.583555, loss: 0.321820\n",
      "val: bce: 0.331692, dice: 0.920304, loss: 0.625998\n",
      "saving best model\n",
      "0m 4s\n",
      "Epoch 36/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.055126, dice: 0.568891, loss: 0.312008\n",
      "val: bce: 0.240127, dice: 0.840080, loss: 0.540103\n",
      "saving best model\n",
      "0m 4s\n",
      "Epoch 37/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.054053, dice: 0.559752, loss: 0.306902\n",
      "val: bce: 0.245065, dice: 0.855981, loss: 0.550523\n",
      "0m 4s\n",
      "Epoch 38/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.053102, dice: 0.553013, loss: 0.303057\n",
      "val: bce: 0.238166, dice: 0.865122, loss: 0.551644\n",
      "0m 4s\n",
      "Epoch 39/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.053642, dice: 0.536132, loss: 0.294887\n",
      "val: bce: 0.260386, dice: 0.905383, loss: 0.582884\n",
      "0m 4s\n",
      "Epoch 40/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.054799, dice: 0.546120, loss: 0.300459\n",
      "val: bce: 0.258129, dice: 0.946595, loss: 0.602362\n",
      "0m 4s\n",
      "Epoch 41/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.052992, dice: 0.534357, loss: 0.293675\n",
      "val: bce: 0.402766, dice: 0.995152, loss: 0.698959\n",
      "0m 4s\n",
      "Epoch 42/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.050093, dice: 0.513476, loss: 0.281784\n",
      "val: bce: 0.359417, dice: 0.981034, loss: 0.670226\n",
      "0m 4s\n",
      "Epoch 43/199\n",
      "----------\n",
      "LR 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: bce: 0.052528, dice: 0.514802, loss: 0.283665\n",
      "val: bce: 0.373618, dice: 0.978592, loss: 0.676105\n",
      "0m 4s\n",
      "Epoch 44/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.047692, dice: 0.496635, loss: 0.272164\n",
      "val: bce: 0.385325, dice: 0.982306, loss: 0.683815\n",
      "0m 4s\n",
      "Epoch 45/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.046037, dice: 0.483521, loss: 0.264779\n",
      "val: bce: 0.390438, dice: 0.993430, loss: 0.691934\n",
      "0m 4s\n",
      "Epoch 46/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.047057, dice: 0.473515, loss: 0.260286\n",
      "val: bce: 0.383781, dice: 0.998165, loss: 0.690973\n",
      "0m 4s\n",
      "Epoch 47/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.048439, dice: 0.453251, loss: 0.250845\n",
      "val: bce: 0.294697, dice: 0.914505, loss: 0.604601\n",
      "0m 4s\n",
      "Epoch 48/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.045627, dice: 0.449227, loss: 0.247427\n",
      "val: bce: 0.168671, dice: 0.652725, loss: 0.410698\n",
      "saving best model\n",
      "0m 4s\n",
      "Epoch 49/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.043766, dice: 0.425392, loss: 0.234579\n",
      "val: bce: 0.157618, dice: 0.698276, loss: 0.427947\n",
      "0m 4s\n",
      "Epoch 50/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.052975, dice: 0.442367, loss: 0.247671\n",
      "val: bce: 0.060454, dice: 0.452650, loss: 0.256552\n",
      "saving best model\n",
      "0m 4s\n",
      "Epoch 51/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.045060, dice: 0.446432, loss: 0.245746\n",
      "val: bce: 0.058549, dice: 0.468228, loss: 0.263388\n",
      "0m 4s\n",
      "Epoch 52/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.043691, dice: 0.427746, loss: 0.235719\n",
      "val: bce: 0.057664, dice: 0.439884, loss: 0.248774\n",
      "saving best model\n",
      "0m 4s\n",
      "Epoch 53/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.056950, dice: 0.425554, loss: 0.241252\n",
      "val: bce: 0.064230, dice: 0.421603, loss: 0.242917\n",
      "saving best model\n",
      "0m 4s\n",
      "Epoch 54/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.043166, dice: 0.392411, loss: 0.217789\n",
      "val: bce: 0.116496, dice: 0.488685, loss: 0.302591\n",
      "0m 4s\n",
      "Epoch 55/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.041576, dice: 0.399629, loss: 0.220602\n",
      "val: bce: 0.168460, dice: 0.596340, loss: 0.382400\n",
      "0m 4s\n",
      "Epoch 56/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.041205, dice: 0.382077, loss: 0.211641\n",
      "val: bce: 0.218718, dice: 0.727127, loss: 0.472923\n",
      "0m 4s\n",
      "Epoch 57/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.045090, dice: 0.376725, loss: 0.210907\n",
      "val: bce: 0.236346, dice: 0.765773, loss: 0.501060\n",
      "0m 4s\n",
      "Epoch 58/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.044782, dice: 0.365397, loss: 0.205090\n",
      "val: bce: 0.264536, dice: 0.816875, loss: 0.540705\n",
      "0m 4s\n",
      "Epoch 59/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.041733, dice: 0.350808, loss: 0.196271\n",
      "val: bce: 0.327332, dice: 0.938052, loss: 0.632692\n",
      "0m 4s\n",
      "Epoch 60/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.041370, dice: 0.338121, loss: 0.189745\n",
      "val: bce: 0.377247, dice: 0.997580, loss: 0.687413\n",
      "0m 4s\n",
      "Epoch 61/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.041448, dice: 0.329796, loss: 0.185622\n",
      "val: bce: 0.372870, dice: 0.997177, loss: 0.685024\n",
      "0m 3s\n",
      "Epoch 62/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.040290, dice: 0.331229, loss: 0.185759\n",
      "val: bce: 0.365682, dice: 0.997057, loss: 0.681369\n",
      "0m 4s\n",
      "Epoch 63/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.042140, dice: 0.326783, loss: 0.184462\n",
      "val: bce: 0.305032, dice: 0.982554, loss: 0.643793\n",
      "0m 4s\n",
      "Epoch 64/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.036308, dice: 0.306940, loss: 0.171624\n",
      "val: bce: 0.205709, dice: 0.713287, loss: 0.459498\n",
      "0m 4s\n",
      "Epoch 65/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.036539, dice: 0.299967, loss: 0.168253\n",
      "val: bce: 0.146958, dice: 0.530761, loss: 0.338860\n",
      "0m 4s\n",
      "Epoch 66/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.035814, dice: 0.301512, loss: 0.168663\n",
      "val: bce: 0.132244, dice: 0.474034, loss: 0.303139\n",
      "0m 4s\n",
      "Epoch 67/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.033883, dice: 0.292542, loss: 0.163212\n",
      "val: bce: 0.130139, dice: 0.462536, loss: 0.296338\n",
      "0m 4s\n",
      "Epoch 68/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.034333, dice: 0.286780, loss: 0.160557\n",
      "val: bce: 0.134642, dice: 0.470487, loss: 0.302564\n",
      "0m 4s\n",
      "Epoch 69/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.033328, dice: 0.273753, loss: 0.153540\n",
      "val: bce: 0.150089, dice: 0.505790, loss: 0.327940\n",
      "0m 4s\n",
      "Epoch 70/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.031979, dice: 0.266576, loss: 0.149278\n",
      "val: bce: 0.161470, dice: 0.533748, loss: 0.347609\n",
      "0m 4s\n",
      "Epoch 71/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.033798, dice: 0.260014, loss: 0.146906\n",
      "val: bce: 0.143550, dice: 0.459546, loss: 0.301548\n",
      "0m 4s\n",
      "Epoch 72/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.032275, dice: 0.270970, loss: 0.151623\n",
      "val: bce: 0.140563, dice: 0.461569, loss: 0.301066\n",
      "0m 4s\n",
      "Epoch 73/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.039051, dice: 0.259530, loss: 0.149290\n",
      "val: bce: 0.101373, dice: 0.364187, loss: 0.232780\n",
      "saving best model\n",
      "0m 4s\n",
      "Epoch 74/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.032915, dice: 0.261846, loss: 0.147380\n",
      "val: bce: 0.127729, dice: 0.410584, loss: 0.269156\n",
      "0m 4s\n",
      "Epoch 75/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.034424, dice: 0.249501, loss: 0.141962\n",
      "val: bce: 0.147172, dice: 0.475066, loss: 0.311119\n",
      "0m 4s\n",
      "Epoch 76/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.033831, dice: 0.244810, loss: 0.139320\n",
      "val: bce: 0.147472, dice: 0.488387, loss: 0.317929\n",
      "0m 4s\n",
      "Epoch 77/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.031874, dice: 0.239126, loss: 0.135500\n",
      "val: bce: 0.117400, dice: 0.399548, loss: 0.258474\n",
      "0m 4s\n",
      "Epoch 78/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.028519, dice: 0.242999, loss: 0.135759\n",
      "val: bce: 0.122063, dice: 0.426096, loss: 0.274080\n",
      "0m 4s\n",
      "Epoch 79/199\n",
      "----------\n",
      "LR 0.05\n",
      "train: bce: 0.029481, dice: 0.237259, loss: 0.133370\n",
      "val: bce: 0.138875, dice: 0.526669, loss: 0.332772\n",
      "0m 4s\n",
      "Epoch 80/199\n",
      "----------\n",
      "LR 0.005000000000000001\n",
      "train: bce: 0.033355, dice: 0.231539, loss: 0.132447\n",
      "val: bce: 0.127084, dice: 0.470404, loss: 0.298744\n",
      "0m 4s\n",
      "Epoch 81/199\n",
      "----------\n",
      "LR 0.005000000000000001\n",
      "train: bce: 0.031651, dice: 0.225578, loss: 0.128614\n",
      "val: bce: 0.113187, dice: 0.414038, loss: 0.263612\n",
      "0m 4s\n",
      "Epoch 82/199\n",
      "----------\n",
      "LR 0.005000000000000001\n",
      "train: bce: 0.032121, dice: 0.224460, loss: 0.128290\n",
      "val: bce: 0.100438, dice: 0.367185, loss: 0.233812\n",
      "0m 4s\n",
      "Epoch 83/199\n",
      "----------\n",
      "LR 0.005000000000000001\n",
      "train: bce: 0.028870, dice: 0.221286, loss: 0.125078\n",
      "val: bce: 0.090704, dice: 0.336458, loss: 0.213581\n",
      "saving best model\n",
      "0m 4s\n",
      "Epoch 84/199\n",
      "----------\n",
      "LR 0.005000000000000001\n",
      "train: bce: 0.028298, dice: 0.223407, loss: 0.125852\n",
      "val: bce: 0.082827, dice: 0.315198, loss: 0.199013\n",
      "saving best model\n",
      "0m 4s\n",
      "Epoch 85/199\n",
      "----------\n",
      "LR 0.005000000000000001\n",
      "train: bce: 0.029009, dice: 0.228974, loss: 0.128991\n",
      "val: bce: 0.076225, dice: 0.299757, loss: 0.187991\n",
      "saving best model\n",
      "0m 4s\n",
      "Epoch 86/199\n",
      "----------\n",
      "LR 0.005000000000000001\n",
      "train: bce: 0.028648, dice: 0.223355, loss: 0.126002\n",
      "val: bce: 0.072565, dice: 0.292787, loss: 0.182676\n",
      "saving best model\n",
      "0m 4s\n",
      "Epoch 87/199\n",
      "----------\n",
      "LR 0.005000000000000001\n",
      "train: bce: 0.027771, dice: 0.224517, loss: 0.126144\n",
      "val: bce: 0.071711, dice: 0.290224, loss: 0.180968\n",
      "saving best model\n",
      "0m 4s\n",
      "Epoch 88/199\n",
      "----------\n",
      "LR 0.005000000000000001\n",
      "train: bce: 0.027666, dice: 0.218337, loss: 0.123001\n",
      "val: bce: 0.071172, dice: 0.288564, loss: 0.179868\n",
      "saving best model\n",
      "0m 4s\n",
      "Epoch 89/199\n",
      "----------\n",
      "LR 0.005000000000000001\n",
      "train: bce: 0.028093, dice: 0.219368, loss: 0.123730\n",
      "val: bce: 0.071532, dice: 0.288289, loss: 0.179911\n",
      "0m 4s\n",
      "Epoch 90/199\n",
      "----------\n",
      "LR 0.005000000000000001\n",
      "train: bce: 0.028782, dice: 0.225907, loss: 0.127344\n",
      "val: bce: 0.071807, dice: 0.288640, loss: 0.180224\n",
      "0m 4s\n",
      "Epoch 91/199\n",
      "----------\n",
      "LR 0.005000000000000001\n",
      "train: bce: 0.028187, dice: 0.214986, loss: 0.121586\n",
      "val: bce: 0.072524, dice: 0.289398, loss: 0.180961\n",
      "0m 4s\n",
      "Epoch 92/199\n",
      "----------\n",
      "LR 0.005000000000000001\n",
      "train: bce: 0.029507, dice: 0.213296, loss: 0.121402\n",
      "val: bce: 0.072115, dice: 0.288565, loss: 0.180340\n",
      "0m 4s\n",
      "Epoch 93/199\n",
      "----------\n",
      "LR 0.005000000000000001\n",
      "train: bce: 0.029380, dice: 0.213404, loss: 0.121392\n",
      "val: bce: 0.070969, dice: 0.286715, loss: 0.178842\n",
      "saving best model\n",
      "0m 4s\n",
      "Epoch 94/199\n",
      "----------\n",
      "LR 0.005000000000000001\n",
      "train: bce: 0.029546, dice: 0.212379, loss: 0.120962\n",
      "val: bce: 0.068824, dice: 0.283990, loss: 0.176407\n",
      "saving best model\n",
      "0m 4s\n",
      "Epoch 95/199\n",
      "----------\n",
      "LR 0.005000000000000001\n",
      "train: bce: 0.029161, dice: 0.215803, loss: 0.122482\n",
      "val: bce: 0.066530, dice: 0.281143, loss: 0.173837\n",
      "saving best model\n",
      "0m 4s\n",
      "Epoch 96/199\n",
      "----------\n",
      "LR 0.005000000000000001\n",
      "train: bce: 0.029190, dice: 0.214610, loss: 0.121900\n",
      "val: bce: 0.064771, dice: 0.279954, loss: 0.172363\n",
      "saving best model\n",
      "0m 4s\n",
      "Epoch 97/199\n",
      "----------\n",
      "LR 0.005000000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: bce: 0.027791, dice: 0.213112, loss: 0.120451\n",
      "val: bce: 0.064052, dice: 0.279378, loss: 0.171715\n",
      "saving best model\n",
      "0m 4s\n",
      "Epoch 98/199\n",
      "----------\n",
      "LR 0.005000000000000001\n",
      "train: bce: 0.027270, dice: 0.211068, loss: 0.119169\n",
      "val: bce: 0.064120, dice: 0.278866, loss: 0.171493\n",
      "saving best model\n",
      "0m 4s\n",
      "Epoch 99/199\n",
      "----------\n",
      "LR 0.005000000000000001\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "subj = '01'\n",
    "version = None\n",
    "n_layers = 5\n",
    "model_cap = 16\n",
    "channel_growth = True\n",
    "seq_mode = 'random'\n",
    "model_arch='3D'\n",
    "now = datetime.today().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "if version is None: version = now\n",
    "#model_version=f'chaos_notebook_test_{version}'\n",
    "model_version=version\n",
    "print(now)\n",
    "inputs, targets, names, model = train_seg_model(data_dir, 'xarray_chaos.nc', out_dir, model_version=model_version, subj=subj, loss='dice', dry_run=False,\n",
    "                                                transform=True, def_seq_mode=seq_mode, coord_conv=False, step_size=80, num_epochs=200, lr=5e-2, \n",
    "                                                model_arch=model_arch, resize=False, n_layers=n_layers, channel_growth=channel_growth,\n",
    "                                                model_cap=model_cap, batch_size=100, test_seq_mode='all', test_aug=False, train_aug=False, val_aug=False,\n",
    "                                                bc_weight=0.2)\n",
    "\n",
    "# model_path = Path(out_dir, 'trained_models', subj, f'model_{model_version}.pkl')\n",
    "# model = pytorch_arch.GeneralUNet3D(n_layers, 1, model_cap, 1, channel_growth, False, False)\n",
    "# model_dict = torch.load(model_path, map_location='cpu')\n",
    "# model_dict = OrderedDict([(key[7:], val) for key, val in model_dict.items()])\n",
    "# model.load_state_dict(model_dict, strict=True)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_pred = None\n",
    "if model:\n",
    "    # inputs.to('cuda:0')\n",
    "    model_pred = torch.zeros_like(inputs)\n",
    "    for i in range(inputs.shape[0]):\n",
    "        for j in range(inputs.shape[1]):\n",
    "            model_pred[i, j, :] = model(inputs[i:i+1, j:j+1, :])\n",
    "            model_pred[i, j, :] = F.sigmoid(model_pred[i, j, :])\n",
    "            ones = torch.ones_like(model_pred[i, j, :])\n",
    "            zeros = torch.zeros_like(model_pred[i, j, :])\n",
    "        # model_pred[:, i, :] = torch.where(model_pred[:, i, :]>3e-3, ones, zeros)\n",
    "    inputs.to('cpu')\n",
    "hv_dl_vis_chaos(inputs, targets, names, ['t1_in', 't1_out', 't2'], model_pred)\n",
    "# hv_dl_vis_chaos(inputs, targets, names, ['seq'], model_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slurm Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path(out_dir, 'config')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for f in list(config_path.glob('*2019-10-03_12-10-03*.pkl')):\n",
    "    s_tmp = pd.Series(pd.read_pickle(str(f)), name=f.stem)\n",
    "    df = df.append(s_tmp, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['test_dice_mean'] = (df.test_dice_t1_in+df.test_dice_t1_out+df.test_dice_t2)/3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.query('channel_growth==1').sort_values('test_dice_t1_out').reset_index().rename(columns={'index':'job_name'})\n",
    "df2 = df.query('channel_growth==0').sort_values('test_dice_t1_out').reset_index().rename(columns={'index':'job_name'})\n",
    "(df1.hvplot.line(x='index', y='test_dice_t1_out', hover_cols=['model_cap', 'def_seq_mode', 'bce_weight'], label='C Growth')*\n",
    "df2.hvplot.line(x='index', y='test_dice_t1_out', hover_cols=['model_cap', 'def_seq_mode', 'bce_weight'], label='C Static')).opts(legend_position='top_left', show_legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.query('def_seq_mode==\"t1_in\"').sort_values('test_dice_t1_out').reset_index().rename(columns={'index':'job_name'})\n",
    "df2 = df.query('def_seq_mode==\"t1_out\"').sort_values('test_dice_t1_out').reset_index().rename(columns={'index':'job_name'})\n",
    "df3 = df.query('def_seq_mode==\"t2\"').sort_values('test_dice_t1_out').reset_index().rename(columns={'index':'job_name'})\n",
    "df4 = df.query('def_seq_mode==\"random\"').sort_values('test_dice_t1_out').reset_index().rename(columns={'index':'job_name'})\n",
    "(df1.hvplot.line(x='index', y='test_dice_t1_out', hover_cols=['model_cap', 'def_seq_mode', 'bce_weight', 'job_name'], label='t1_in')*\n",
    "df2.hvplot.line(x='index', y='test_dice_t1_out', hover_cols=['model_cap', 'def_seq_mode', 'bce_weight', 'job_name'], label='t1_out')*\n",
    "df3.hvplot.line(x='index', y='test_dice_t1_out', hover_cols=['model_cap', 'def_seq_mode', 'bce_weight', 'job_name'], label='t2')*\n",
    "df4.hvplot.line(x='index', y='test_dice_t1_out', hover_cols=['model_cap', 'def_seq_mode', 'bce_weight', 'job_name'], label='random')\n",
    ").opts(legend_position='top_left', show_legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.query('bce_weight==0.2').sort_values('best_loss').reset_index().rename(columns={'index':'job_name'})\n",
    "df3 = df.query('bce_weight==0.5').sort_values('best_loss').reset_index().rename(columns={'index':'job_name'})\n",
    "df4 = df.query('bce_weight==0.8').sort_values('best_loss').reset_index().rename(columns={'index':'job_name'})\n",
    "(df1.hvplot.line(x='index', y='best_loss', hover_cols=['model_cap', 'def_seq_mode', 'bce_weight'], label='0.2')*\n",
    "df3.hvplot.line(x='index', y='best_loss', hover_cols=['model_cap', 'def_seq_mode', 'bce_weight'], label='0.5')*\n",
    "df4.hvplot.line(x='index', y='best_loss', hover_cols=['model_cap', 'def_seq_mode', 'bce_weight'], label='0.8')\n",
    ").opts(legend_position='top_left', show_legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "df0 = df.query('model_cap==4').sort_values('test_dice_mean').reset_index().rename(columns={'index':'job_name'})\n",
    "df1 = df.query('model_cap==8').sort_values('test_dice_mean').reset_index().rename(columns={'index':'job_name'})\n",
    "df2 = df.query('model_cap==12').sort_values('test_dice_mean').reset_index().rename(columns={'index':'job_name'})\n",
    "df3 = df.query('model_cap==16').sort_values('test_dice_mean').reset_index().rename(columns={'index':'job_name'})\n",
    "df4 = df.query('model_cap==32').sort_values('test_dice_mean').reset_index().rename(columns={'index':'job_name'})\n",
    "(df0.hvplot.line(x='index', y='test_dice_mean', hover_cols=['model_cap', 'def_seq_mode', 'bce_weight'], label='4')*\n",
    "df1.hvplot.line(x='index', y='test_dice_mean', hover_cols=['model_cap', 'def_seq_mode', 'bce_weight'], label='8')*\n",
    "df2.hvplot.line(x='index', y='test_dice_mean', hover_cols=['model_cap', 'def_seq_mode', 'bce_weight'], label='12')*\n",
    "df3.hvplot.line(x='index', y='test_dice_mean', hover_cols=['model_cap', 'def_seq_mode', 'bce_weight'], label='16')*\n",
    "df4.hvplot.line(x='index', y='test_dice_mean', hover_cols=['model_cap', 'def_seq_mode', 'bce_weight'], label='32')\n",
    ").opts(legend_position='top_left', show_legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['model_cap', 'def_seq_mode'])['test_dice_t1_out'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: t1_out seems to outperform all other combos (including random).  Best current overall: t1_out, model_cap=8.  Why would adding additional images decrease performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df.query('model_cap==8 and def_seq_mode==\"t1_out\" and subj==\"01\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.query('n_layers==5').sort_values('test_dice_t1_out').reset_index().rename(columns={'index':'job_name'})\n",
    "df2 = df.query('n_layers==6').sort_values('test_dice_t1_out').reset_index().rename(columns={'index':'job_name'})\n",
    "df3 = df.query('n_layers==7').sort_values('test_dice_t1_out').reset_index().rename(columns={'index':'job_name'})\n",
    "(df1.hvplot.line(x='index', y='test_dice_t1_out', hover_cols=['model_cap', 'def_seq_mode', 'bce_weight'], label='5')*\n",
    "df2.hvplot.line(x='index', y='test_dice_t1_out', hover_cols=['model_cap', 'def_seq_mode', 'bce_weight'], label='6')*\n",
    "df3.hvplot.line(x='index', y='test_dice_t1_out', hover_cols=['model_cap', 'def_seq_mode', 'bce_weight'], label='7')\n",
    ").opts(legend_position='top_left', show_legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
